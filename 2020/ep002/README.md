# Journal Club, episode 2

Kyle discussed Google's recent open sourcing of [ALBERT](https://www.infoq.com/news/2020/01/google-albert-ai-nlp/), a variant of the famous BERT model for natural language processing.  ALBERT is more compact and uses fewer parameters.

George lead a discussion about the paper [Explainable Artificial Intelligence: Understanding, visualizing, and interpreting deep learning models](https://arxiv.org/abs/1708.08296) by Samek, Wiegand, and Muller. This work introduces two tools for generating local interpretability and a novel metric to objectively compare the quality of explanations.

Lan talked about her experience [generating new Seinfeld](https://dataskeptic.com/blog/2020/nlp/gpt2-seinfield) scripts using GPT-2.
